{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping all the features\n",
    "def get_all_specs(soup:BeautifulSoup) -> str:\n",
    "    return soup.find(\"div\", attrs = {\"id\": \"specs\"})\n",
    "\n",
    "#Putting all the features into a dictionary\n",
    "def get_all_features(soup: BeautifulSoup) -> dict:\n",
    "    features = {}    \n",
    "    all_specs = get_all_specs(soup).find_all(\"ul\") if get_all_specs(soup) else \"No specs\"\n",
    "    if all_specs:\n",
    "        for ul_element in all_specs:\n",
    "            list_items = ul_element.find_all(\"li\")  \n",
    "\n",
    "            # val_check = check.find(\"span\", attrs )    \n",
    "            for li in list_items:\n",
    "                \n",
    "                value_element = li.find(\"span\", attrs = {\"class\": \"list__value\"})\n",
    "                key_element = li.find(\"span\", attrs = {\"class\": \"list__key\"})\n",
    "                \n",
    "                \n",
    "                check = li.find(\"span\", attrs = {\"class\": \"list__value list__value--large\"}) if li.find(\"span\", attrs = {\"class\": \"list__value list__value--large\"}) else None\n",
    "                if check:\n",
    "                    features[\"Original price\"] = check.text.strip()\n",
    "                    features[\"TAXES PAID\"] = li.find(\"span\", attrs = {\"class\": \"list__key\"}).text.strip() if li.find(\"span\", attrs = {\"class\": \"list__key\"}) else None\n",
    "                    continue\n",
    "                if key_element is None:\n",
    "                    value = value_element.text.strip()\n",
    "                    features[\"Price\"] = value\n",
    "                    \n",
    "                    \n",
    "                if key_element and value_element:\n",
    "                    key = key_element.text.strip()\n",
    "                    value = value_element.text.strip()\n",
    "                    features[key] = value\n",
    "            \n",
    "    features[\"Location\"] = get_location(soup)\n",
    "    features[\"Description\"] = get_description(soup)\n",
    "    features[\"Categories\"] = get_categories(soup)\n",
    "    features.update(get_equipment(soup)) if get_equipment(soup) else None\n",
    "    features[\"VAT\"] = get_vat(soup)\n",
    "    features[\"Views, favourites, ID and listings\"] = get_id(soup)\n",
    "    return features\n",
    "            \n",
    "        \n",
    "\n",
    "#Getting the selling location of the boat\n",
    "def get_location(soup: BeautifulSoup) -> str:\n",
    "    temp = soup.find(\"div\", attrs = {\"id\": \"location\"}) if soup.find(\"div\", attrs = {\"id\": \"location\"}) else \"No location\"\n",
    "    return temp.find(\"p\", attrs = {\"class\": \"text\"}).text.strip()\n",
    "    \n",
    "\n",
    "#Each page has a list of boats, this function extracts the URL of each boat\n",
    "def get_individual_URL(soup: BeautifulSoup) -> list:\n",
    "    l = []\n",
    "    links = soup.find_all(\"h3\", attrs = {\"class\": \"blurb__title\"})\n",
    "    for link in links:\n",
    "        l.append(link.find(\"a\")[\"href\"])\n",
    "        \n",
    "    return l\n",
    "\n",
    "#Extracting the preview image of the boat\n",
    "def get_image(soup: BeautifulSoup) -> str:\n",
    "    pictures =  soup.find_all(\"picture\")\n",
    "    return pictures[0].find(\"img\")[\"src\"] if pictures else \"No image\"\n",
    "\n",
    "#Extracting all the images of the boat\n",
    "def all_images(soup: BeautifulSoup) -> list:\n",
    "    pictures =  soup.find_all(\"picture\")\n",
    "    l = []\n",
    "    for picture in pictures:\n",
    "        imgs = picture.find_all(\"img\")\n",
    "        for img in imgs:\n",
    "            # Access the 'src' attribute of each img\n",
    "            img_src = img.get(\"src\")  # Use .get() for safer access\n",
    "            # Check if the src attribute ends with '.jpg' and is not None\n",
    "            if img_src and img_src.endswith(\".jpg\"):\n",
    "                l.append(img_src)\n",
    "    \n",
    "    \n",
    "    #getting all remaining pictures        \n",
    "    k = 3\n",
    "    \n",
    "    while True:      \n",
    "        img_element = soup.find(\"li\", attrs={\"data-image-id\": f\"image{k}\"})\n",
    "        if not img_element:\n",
    "            break \n",
    "        img = img_element.find(\"img\")\n",
    "        if img:\n",
    "            srcset = img.get('srcset') or img.get('data-srcset')\n",
    "            if srcset:\n",
    "                links = [link.strip().split(' ')[0] for link in srcset.split(',')]\n",
    "                if len(links) >= 2:\n",
    "                    l.append(links[1])\n",
    "        k += 1  \n",
    "    return l if pictures else \"No images\"\n",
    "\n",
    "\n",
    "\n",
    "#Extracting the description of the boat\n",
    "def get_description(soup: BeautifulSoup) -> str:\n",
    "    return soup.find(\"div\", attrs = {\"class\": \"content\"}).text.strip() if soup.find(\"div\", attrs = {\"class\": \"content\"}) else \"No description\"\n",
    "\n",
    "#Getting the categories\n",
    "\n",
    "def get_categories(soup: BeautifulSoup) -> list:\n",
    "    temp = soup.find(\"div\", attrs = {\"id\": \"specs\"})\n",
    "    for t in temp:\n",
    "        if t.find(\"p\"):\n",
    "            return t.find(\"p\").text.split(\",\")\n",
    "    return None\n",
    "\n",
    "#Get likes, favourites, ID and listing date\n",
    "def get_id(soup: BeautifulSoup) -> list:\n",
    "    id = []\n",
    "    l = soup.find_all(\"li\") if soup.find_all(\"li\") else \"No ID\"\n",
    "    for k in l:\n",
    "        if k.find(\"strong\"):\n",
    "            id.append(k.find(\"strong\").text)\n",
    "\n",
    "    return id    \n",
    "            \n",
    "\n",
    "\n",
    "#Get the equipment that is included with the boat\n",
    "\n",
    "def get_equipment(soup: BeautifulSoup) -> dict:\n",
    "    temp = soup.find(\"div\", attrs={\"id\": \"equipment\"})\n",
    "    all_equipment = temp.find_all(\"li\") if temp else \"No equipment\"\n",
    "    equipment = {k.text: 1 for k in all_equipment} if temp else None\n",
    "    return equipment \n",
    "    \n",
    "\n",
    "#Get the VAT status of the boat\n",
    "def get_vat(soup: BeautifulSoup) -> str:\n",
    "    vat = soup.find(\"span\", attrs = {\"class\": \"list__key\"}).text\n",
    "    return vat if vat else \"No VAT\"\n",
    "\n",
    "\n",
    "def find_id(soup: BeautifulSoup) -> str:\n",
    "    all = soup.find(\"ul\", attrs = {\"class\": \"list list--space-8 l-mb-32\"})\n",
    "    for a in all:\n",
    "        if a.find(\"strong\"):\n",
    "            return a.find(\"strong\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ({'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\", \"Accept-Language\": \"en-UK, en;q=0.5\"})\n",
    "URL = f\"https://boat24.com/uk/sailingboats/beneteau/beneteau-57/detail/564009/\"\n",
    "webpage = requests.get(URL, headers = HEADERS)\n",
    "\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NO MULTITHREADING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ({'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\", \"Accept-Language\": \"en-UK, en;q=0.5\"})\n",
    "k = 1\n",
    "def scrape(i: int, all_descriptors:dict, images:dict):\n",
    "    try:\n",
    "        print(i)\n",
    "        global k\n",
    "        \n",
    "        # TIMES 20 BECAUSE OF PAGE NUMBER\n",
    "        URL =f\"https://www.boat24.com/uk/secondhandboats/?page={i*20}\"\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        \n",
    "        links = get_individual_URL(soup)\n",
    "        \n",
    "        for link in links:\n",
    "            \n",
    "            URL = link\n",
    "            webpage = requests.get(URL, headers=HEADERS)\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "            specs = get_all_features(soup)\n",
    "            if specs is not None:\n",
    "                all_descriptors[f\"Boat_{k}\"] = specs\n",
    "                all_descriptors[f\"Boat_{k}\"][\"URL\"] = URL\n",
    "                images[f\"Boat_{k}\"] = all_images(soup)\n",
    "                # print(k)\n",
    "                k += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping boat {i}: {e}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "def scrape_images(i: int, images: dict):\n",
    "    print(i)\n",
    "    # global k\n",
    "    URL =f\"https://www.boat24.com/uk/secondhandboats/?page={i*20}\"\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    print(soup)\n",
    "    links = get_individual_URL(soup)\n",
    "    # print(links)\n",
    "    for link in links:\n",
    "        URL = link\n",
    "        webpage = requests.get(URL, headers=HEADERS)\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "        key = find_id(soup)\n",
    "        get_all_images = all_images(soup)\n",
    "        if key not in images:\n",
    "            images[key] = get_all_images\n",
    "        #     k += 1\n",
    "        # k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING WITH MULTITHREADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ({'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\", \"Accept-Language\": \"en-UK, en;q=0.5\"})\n",
    "\n",
    "all_descriptors = {}\n",
    "images = {}\n",
    "k = 1\n",
    "\n",
    "\n",
    "num_pages = 1295\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    tasks = []\n",
    "    for i in range(num_pages):\n",
    "        tasks.append(executor.submit(scrape, i, all_descriptors, images))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(tasks):\n",
    "        future.result()\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lists = pd.DataFrame.from_dict(images, orient='index')\n",
    "\n",
    "# Optionally, reset the index if you want the keys in a separate column instead of as row indices\n",
    "df_lists_reset = df_lists.reset_index()\n",
    "\n",
    "# Rename columns as needed\n",
    "df_lists_reset.columns = ['Key'] + [f'image {i}' for i in range(1, len(df_lists_reset.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = df_lists_reset[[\"Key\", \"image 1\", \"image 2\", \"image 3\", \"image 4\", \"image 5\", \"image 6\", \"image 7\", \"image 8\", \"image 9\", \"image 10\", \"image 11\", \"image 12\", \"image 13\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_descriptors).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df.merge(keep, left_on=\"ID\", right_on=\"Key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_path = \"../Data/For_henry/full_data_27k.csv\"\n",
    "merged_df.to_csv(merge_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
